{
 "metadata": {
  "name": "",
  "signature": "sha256:f0c0af090806acf34ccbcb4044d7a31327a968b249354534579d5d4efddb9724"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "C4L15 Social"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## You'll need to install these modules if you want to use them.\n",
      "e.g. From the command line: pip install matplotlib"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib as plt\n",
      "import networkx as nx\n",
      "plt.rc('figure', figsize=(8, 5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Twitter\n",
      "Borrowing from the example at http://nbviewer.ipython.org/gist/ellisonbg/3837783. \n",
      "### Twitter API now requires OAuth2 for searching.\n",
      "My example below updates ellisonbg's example to accomodate Twitter's OAuth2 requirement.\n",
      "Twython makes it easy. You just need to get an APP_KEY and APP_SECRET from Twitter.\n",
      "See https://twython.readthedocs.org/en/latest/usage/starting_out.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Secrets!\n",
      "APP_KEY = ''\n",
      "APP_SECRET = ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from twython import Twython\n",
      "twitter = Twython(APP_KEY, APP_SECRET, oauth_version=2)\n",
      "ACCESS_TOKEN = twitter.obtain_access_token()\n",
      "twitter = Twython(APP_KEY, access_token=ACCESS_TOKEN)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Search Twitter returning pages of 20 tweets.\n",
      "max_id is used to to set an upper bound on the time of the most\n",
      "recent tweet returned. The first query is run without max_id, then\n",
      "the last (earliest) tweet from the result set is used as the max_id value for the next\n",
      "set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "query = \"%23c4l15\"\n",
      "n_pages = 100\n",
      "results = []\n",
      "retweets = []\n",
      "urls = []\n",
      "max_id_next_page = 0\n",
      "# First query without max_id\n",
      "search = twitter.search(q=query, lang='en', count=20)\n",
      "for page in range(1, n_pages+1):\n",
      "    res = search['statuses']\n",
      "    # With max_id set, the first result will\n",
      "    # be a repeat of the last result from the previous query.\n",
      "    # Delete it.\n",
      "    if max_id_next_page != 0:\n",
      "      res = res[1:]\n",
      "    \n",
      "    if not res:\n",
      "        print 'Stopping at page:', page\n",
      "        break\n",
      "        \n",
      "    for t in res:\n",
      "        if t['text'].startswith('RT '):\n",
      "            retweets.append(t)\n",
      "        else:\n",
      "            results.append(t)\n",
      "        for u in t['entities']['urls']:\n",
      "            urls.append(u['expanded_url'])\n",
      "    earliest_result = res[len(res)-1]\n",
      "    max_id_next_page = earliest_result['id']\n",
      "    search = twitter.search(q=query, lang='en', count=20, max_id=max_id_next_page)\n",
      "    \n",
      "tweets = [t['text'] for t in results]\n",
      "urls_uniq = set(urls)\n",
      "# Quick summary\n",
      "print 'query: ', query\n",
      "print 'results: ', len(results)\n",
      "print 'retweets: ', len(retweets)\n",
      "print 'unique urls: ', len(urls_uniq)\n",
      "print 'urls mentioned: ', urls_uniq\n",
      "print 'tweets: ', tweets\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = ([tweet['text'].encode('utf-8') for tweet in results])\n",
      "lines_joined = '\\n'.join(lines)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK includes various tokenizers, but they split punctuation from words. Not so helpful with URLs, hashtags, and user handles:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from nltk.tokenize import TreebankWordTokenizer\n",
      "#tokenizer = TreebankWordTokenizer()\n",
      "#tokens = tokenizer.tokenize(lines_joined)\n",
      "#print tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from nltk.tokenize import PunktWordTokenizer\n",
      "#tokenizer = PunktWordTokenizer()\n",
      "#tokenizer.tokenize(lines_joined)\n",
      "#print tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from nltk.tokenize import WordPunctTokenizer\n",
      "#word_punct_tokenizer = WordPunctTokenizer()\n",
      "#word_punct_tokenizer.tokenize(lines_joined)\n",
      "#print tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Just split on whitespace instead"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "words = lines_joined.split()\n",
      "words = words\n",
      "print len(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stop_words_url = 'http://www.textfixer.com/resources/common-english-words.txt'\n",
      "import urllib2\n",
      "response = urllib2.urlopen(stop_words_url)\n",
      "html = response.read()\n",
      "stop_words = html.split(',')\n",
      "print 'Stop words count:', len(stop_words)\n",
      "def stop_words_filter(words, stop_word):\n",
      "    try:\n",
      "        words = [words.remove(stop_word) for w in words]\n",
      "    except ValueError:\n",
      "        pass\n",
      "    return words\n",
      "# Recursively remove stop_words\n",
      "words_filtered = [stop_words_filter(words, sw) for sw in stop_words]\n",
      "# Flatten\n",
      "words_filtered = [val for sublist in words_filtered for val in sublist]\n",
      "print 'Words with stop words removed:', len(words_filtered)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print words_filtered"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}